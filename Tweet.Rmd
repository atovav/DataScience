---
title: "Análisis de Tweets de AMLO"
author: "Alan"
date: "June 10, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown
```{r ini, echo=FALSE}
library(rtweet)
library(tidyverse)
library(tm)
library(purrr)
library(broom)
library(ggthemes)
library(RColorBrewer)
library(viridis)
rt <- load("Tweets623.rds")
```

Análisis de tweets de AMLO de un periodo de 11 hrs el 6 de Junio 2019.

Prueba de Data Mining en Twitter usando el paquete rtweet.
Se tomo una muestra de 17,077 tweets los cuales tuvieran la palabra o estuvieran referenciados a @opezobrador_, por un periodo de las 7am hasta las 6 pm.  
Para bajar los tweets se necesita una cuenta de twitter y solo se permite bajar 18,000 tweets cada 15 minutos.
Al bajar los Tweets se obtiene lo siguiente:

```{r datos}
head(rt)

```

```{r datamanip, echo=FALSE}

```

Para analizar los tweets, se deben de quitar cosas como emojis, URL, tweets de noticias asi como palabras comunes (ej. asi, de, a, etc.). Después se tienen que separar los tweets en palabras individuales lo cuales se logran con el paquete tidytext. Las 10 palabras mas usadas son: 


```{r top10}
tidy_tweets %>% count(word) %>% top_n(10, n) %>% mutate(word = reorder(word, n)) %>% arrange(desc(n))
```

Vemos que en el top 10, 6 van hacia otros usuarios, mientras "presidente"" es la primera palabra la cual no empieza con @. Visualizando la diferencia de palabras, tenemos la siguiente diferencia:

```{r plot1}
tidy_tweets %>% na.omit() %>% count(word, sort = TRUE) %>% head(20) %>% mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n, fill = word)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_color_brewer(palette = "Spectral")
```

Eliminando todas las palabras que empiezen con @ tenemos lo siguiente:

```{r plot2, echo=FALSE}
tidy_tweets %>% na.omit() %>% filter(!str_detect(word, "^@")) %>% count(word, sort = TRUE) %>% head(20) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n, fill = word)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_color_brewer(palette = "Spectral")
```
Tenemos una mejor vista de las palabras más usadas en este periodo. Se notan términos como corrupción, migrantes, seguridad y aranceles que van acorde con los principales temas que están sucediendo al momento de analizar estos tweets.

Ahora veremos que palabras cambian rápidamente en el periodo de 11 horas. O para decirlo de otra manera, ¿Que palabras se usan más o menos a lo largo del día? Para realizar esto, tenemos que definir contenedores de una hora y contar el número de palabras usadas dentro de esos contenedores y solo se usaran palabras que sean usadas un mínimo de 30 veces.

```{r}
words_by_time <- tidy_tweets %>%
  filter(!str_detect(word, "^@")) %>%
  mutate(time_floor = floor_date(created_at, unit = "1 hour")) %>%
  count(time_floor, word) %>%
  group_by(time_floor) %>%
  mutate(time_total = sum(n)) %>%
  group_by(word) %>%
  mutate(word_total = sum(n)) %>%
  ungroup() %>%
  rename(count = n) %>%
  filter(word_total > 30)
words_by_time
```

La columna count nos indica las veces que se usó esa palabra en el contenedor de una hora, time_total nos dice cuantas palabras se usaron en ese periodo de tiempo y word_total indica cuantas veces se usó esa palabra en las 11 horas.

Tenemos que realizar un modelo general lineal para cada palabra. Estos modelos responden la pregunta ¿Fue esta palabra mencionada en algún contenedor? ¿Si o No? ¿Como el número de menciones de una palabra depende con el tiempo? Para realizar esto, para cada palabra se adjuntan los datos de cada contenedor y se adjunta el modelo generado.
```{r}
nested_data <- words_by_time %>%
  nest(-word)
nested_models <- nested_data %>%
  mutate(models = map(data, ~ glm(cbind(count, time_total) ~ time_floor, ., 
                                  family = "binomial")))
nested_models
```
El resultado contiene la palabra, la columna data contiene los datos con los contenedores y la columna models contiene el modelo que corresponde a cada palabra.

Posteriormente extraemos las pendientes de cada modelo y buscamos las más importantes. Estamos comparando muchas pendientes por lo cual se aplica un ajuste a los valores p para realizar comparaciones múltiples, y se filtra para encontrar las pendientes más significativas.

```{r}
slopes <- nested_models %>%
  unnest(map(models, tidy)) %>%
  filter(term == "time_floor") %>%
  mutate(adjusted.p.value = p.adjust(p.value))
top_slopes <- slopes %>% 
  filter(adjusted.p.value < 0.05)
top_slopes
```

Tenemos que visualizar los resultados por lo que graficamos a lo largo de las 11 horas y obtenemos la siguiente grafica.
```{r Plottime}
words_by_time %>% inner_join(top_slopes, by = c("word")) %>% filter(!word %in% c("méxico","gt")) %>%
  mutate(freq = count/time_total) %>%
  mutate(word = reorder(word,freq)) %>%
  ggplot(aes(time_floor, freq, color = word)) +
  geom_line(size = 1.3) +
  geom_point() +
  labs(x = NULL, y = "Frequencia") +
  theme_fivethirtyeight() +
  scale_color_viridis(discrete=TRUE, option = "plasma")
```

El mayor cambio resulta sobre la palabra de los aranceles el cual de ser la palabra más usada al inicio queda en tercer lugar. Y se puede ver claramente como cambia el tópico a el universitario que fue asesinado siendo palabras relacionadas a este tema con una pendiente positiva.

Este fue un pequeño análisis, se pretende hacer otro en el cual se incluya análisis de sentimientos para clasificar tweets en positivos y negativos.
